MEMO

# DB Migration 

local to docker container
docker container to local


migrate create -ext sql -dir db/migration -seq init_schema
-seq means migrate version name.

migrate -path db/migration -database "postgres://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose up


brew install postgresql
brew install tableplus
brew install sqlc // sql to go 

# pq - A pure Go postgres driver for Go's database/sql package
go get github.com/lib/pq
go get github.com/stretchr/testify
                                  /require

# CRUD sqlc

move to root directory
 -> command "sqlc init"
 -> set sqlc.yaml file
 -> command "sqlc generate"


# Unit tests

When writing Unit tests,
Should make sure that they are independent from each other.
Why: hard to maintain if has hundred of tests that depends on each other.

# Transaction

DB Transaction

Why?
1. To provide a reliable and consistent unit of work, even in case of system failure.
2. To provide isolation between programs that access the database concurrently.

A: Atomicity
C: Consistency
I: Isolation
D: Durability

Closure is often used when want to get the result from a callback function.
the callback function itself doesn't know the exact type of the result it should return

# Caution when Database Transaction
concurrency carefully!
The best way that is run it with several concurrent go routines

# Update Account within Transaction
Require careful handing of concurrent transactions to avoid deadlock
database locking 

# TDD | test driven development
Tests first to make our current code breaks

# Deadlock postgresql
https://wiki.postgresql.org/wiki/Lock_Monitoring
Deadlock ocuurs because 2 concurrent transactions both need to wait for each other

can resolve deadlock consider order of transactions

SELECT * FROM accounts WHERE id = $1 LIMIT 1
+ FOR NO KEY UPDATE

# Isolation level
dirty read | read uncommitted 
non-repeatable read |read committed
phantom read | repeatable read
serialization anomaly | serializable

postgresql tx begin first when change isolation level.
postgresql not working level of dirty read | read uncommitted.
postgresql uses a dependencies checking mechanism
to detect potensial read phenomena and stop them by throwing error

# Github Actions | workflows
Golang Unit tests in external postgres service in github using .github/workflows/ci.yml

project folder -> 
    mkdir -p .github/workflows
    touch .github/workflows/ci.yml


# HTTP API

Gin.
the router field(package) is private so that Start function makes to access api package

* _ "github.com/lib/pq"

# Viper
golang packege, dealing with configuration file
ex) ENV, YAML ...

app.env
UPPERCASE=123

# Mock DB

make store to mock using interface.
interface cannot be pointer  

gomock makes fake DB Unit tests easily.
through the interface about DB service, mockgen makes Mock Service for Unit tests

# Test Multiple senarios
using t.Run()

# struct `` options -> nedd validator
binding:"oneof= " -> How to avoid hard-coding -> just 'currency'

import "github.com/go-playground/validator/v10"
var validCurreny validator.Func
  -> binding.Validator.Engine() (server.go)

# User Authentication and Authorization

# Update DB schema to previous version 

add foreign key and unique constraint
ref: 
    > many to one
    < one to many
    - one to one

make unique constraint
  indexes
  (field1, field2) [unique] 

Right way to migrate!
if you want to apply a new schema change,
create a new migration version.

command "migrate -verbose up 1 || down 1" -> choose version when migration.
so that can backup the db version

# handle DB errors for adding users migration

change owner -> user.Username(foreign key) to account test file.
command "make mock" regenerate
  -> added User interface to mock store file

Handle error code to the others
400 403 404
500

500 -> client accesss db constraits error code -> 403 

# bcrypt
make hashed password

# gomcok matcher

hash and salt makes pw test difficult
using matcher interface can make custom matcher

just implemented, unit test be stronger 

# JWT To PASETO

JSON Web Token
"Header.Payload.Signature"

JWT Signing Algorithms

1. Symmetric digital signature algorithm
- the same secret key is used to sign & verify Token
- For local use: internal services, where the secret key can be shared
- HS256, HS384, HS512
  - HS256 = HMAC + SHA256
  - HMAC: Hash-based Message Authentication Code
  - SHA: Secure Hash Algorithm
  - 256/384/512: number of output bits

2. Asymmetric disigtal signature algorithm
- The private key is used to sign Token
- The publi key is used to verify Token
- For public use: internal service signs token, but external service needs to verify it
- RS256, RS384, RS512 || PS256, PS384, PS512 || ES ...
  - RS256 = RSA PKCSv1.5 + SHA256 [PKCS: Public-Key cryptography Standards]
  - PS256 = RSA PSS + SHA256 [PSS: Probabilistic Signature Schema]
  - ES256 = ECDSA + SHA256 [ECDSA: Elliptic CUrve Digital Signature Algorithm]

Problem of JWT
  - weak Algorithms
  - Trivial Forgery(위조)


  PASETO
  Platform-Agnostic SEcurity TOkens

  - Stronger algorithm 
  (only need to select the version of PASETO)
  (Only 2 mosst recenbt PASETO versions are accepted)

  local -> Symmetric
  public -> Asymmetric
  
# Create JWT & PASETO token

interface 2 method

CreateToken
VerifyToken

go get github.com/google/uuid

Implementation of interface
  - Add Method for interface
  - Where the struct required function(method) of the interface
  - Implement method
  - Write func () MethodName() {}

Keyfunc is that receives the parsed but unverified token
verify its header, about siging algorithm matches

Test JWTToken function
both happy and error case

jwt to paseto
go get github.com/o1egl/paseto

PASETO CreaetToken -> paseto.Encrypt
PASETO VerifyToken -> paseto.Decrypt

# Login API with token

Add token config
For test, 
make newTestServer with token maker instead of call NewServer

In loginUser,
sensitive data inside db.User struct -> function Upper to lower

# Authentication middleware and Authorization rules

Authorization in Header (API)
access-token belongs specipic user, should not be able to access other users

Using Gin Middleware (Authorization)
  ! -> context.Abort()
  ok -> action api func

Ways
  1. Extract Authorization header from the request
  2. 

  *Multiple Test

    testCases := []struct {
      name          string
      setupAuth     func(t *testing.T, request *http.Request, tokenMaker token.Maker)
      checkResponse func(t *testing.T, recorder *httptest.ResponseRecorder)
      ...
    }{}

    for i := range testCases {

      tc := testCases[i]
      t.Run(tc.name, func(t *testing.T) {
        server := newTestServer(t, nil)
        ...
      })
    }

Gin Router Group
  -> .Group("/path").Use(middleware func)
  Add middleware all of routes in the group

* gomock test rules
buildStubs: func(store *mockdb.MockStore) {
  // error point
  store.EXPECT().GetAccount(gomock.Any(), gomock.Eq(account3.ID)).Times(1).Return(account3, nil)
  // after db times to 0
  store.EXPECT().GetAccount(gomock.Any(), gomock.Eq(account2.ID)).Times(0)
  store.EXPECT().TransferTx(gomock.Any(), gomock.Any()).Times(0)
},

# Add docker

1. git

git checkout -b "new_branch"
master -> new branch -> update code -> test
-> merge to master!

2. mac(local) and progran package version check

3. Dockerfile
make Dockerfile in root
to define the base image to build our app
(based Golang -> needs Golang image)

alpine is small image

    FROM golang:version 
      -> to specify the base image

    WORKDIR /app
      -> to declare the current working directory inside the image

    COPY . .
      -> first, copy the current folder files
      -> second paste to the WORKDIR setting path (place to store)

    RUN go build -o main main.go
      -> build our app to a single binary exec file
      -> -o is setting output file

    EXPOSE 0000
      -> the container listens on the specified network port at runtime
      -> 0000 -> port number
      -> noting that the EXPOSE doesn't actually publish the port

    CMD ["/app/main"]
      -> to define the default command to run when the container starts

  *How to make more smaller?*
  Multistage
  # Build stage(first stage)
    FROM + AS builder
  # Run stage(second stage)
    FROM alpine:3.13
    WORKDIR /app
    COPY --from=builder /app/main . # first stage's (--from is option)

  *Make Image*
  docker build -t image:latest .

  
# Docker Network to connect container

  *Run Image*
  docker run --name app -p PORT:PORT image:latest

  config file .env -> development to production config file
  COPY app.env .

  docker run --name app -p PORT:PORT -e GIN_MODE=release image:latest

To allow 2 stand-alone containers to talk to each other by names
 
  *Golang & PostgreSQL container network setting is different (about localhost)
  docker container inspect container_name
  172.17.0.3 / 172.17.0.2

  but when rebuild image, it IP address will change
  So that the better way is do not rebuild
  -> using viper to read the config
  -> override config file 
  -> Add command into docker run : -e DB_SOURCE="postgres://user:pw@Inspect_IP:PORT/app?sslmode=disable"

  *but when rerun the new container, IP address will change
  So that using user-defined network instead!

  -> check networks
  docker network ls
  docker network inspect bridge

  brigge is offering default IP to container
  -> to create own network
  docker network create new_network_name
  docker network connect new_network_name
  -> docker run app
  -> --network created_network_name
  -> IP address changes to container_name (same network's container name)
  docker run --name app --network network_name -p PORT:PORT -e GIN_MODE=release -e DB_SOURCE="postgres://user:pw@container_name:PORT/app?sslmode=disable" image:latest
  
  now using its name instead of the IP address.

  config file: Inspect_IP -> localhost

* Reviews Code in Github
In Pull requests, add comment about some specific code lines

# How to write docker-compose.yml 

- follow file reference Version 3

version: "3.9"
services:
  postgres:
    image: postgres:12-alpine
    environment:
      - POSTGRES_USER=root
      ...

  // service name
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - localhost to name connected the same network (postgres)
      - DB_SOURCE=postgres...

  - docker compose up
    -> Network simplebank_default       Create...
    -> New network includes postgres and api service 
  - docker compose down 
    -> down container and remove images

  - Edit Dockerfile and docker-compose.yml for migration
  specially Dockerfile Add RUN command "apk add curl" for that

  - Add start.sh in root
  chmod +x start.sh

  #!/bin/sh
  # will be run by /bin/sh
  # alpine image
  # bash is not available

  set -e

  echo "run db migration"
  /app/migrate -path /app/migration -database "$DB_SOURCE" -verbose up

  echo "start the app"
  # takes all parameters passed to the script and run it 
  exec "$@"

  *CMD & ENTRYPOINT (CMD in Dockerfile reference)
  : if CMS id used to provide default arguments for the ENTRYPOINT instruction, 
    both the CMD and ENTRYPOINT instructions should be specified woth the JSON array format

  - docker compose up again (Error handling)
    -> the postgres server was not ready to accept connection with api service yet
    -> Need to wait in docker-compose.yml (depends_on:)

  *depends_on (Control startup and shutdown order in Compose)
  : does not wait for db and redis to be "ready" before starting web only until they have been started.

  - wait-for
    -> COPY wait-for . (Dockerfile)
    -> entrypoint: [ "/app/wait-for", "db:port", "--", "/app/start.sh"] (docker-compose.yml)
       command: [ "/app/main" ]